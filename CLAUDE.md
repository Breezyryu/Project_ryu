# CLAUDE.md - PINN Project Implementation Guidelines

This document provides implementation guidelines and best practices for developing the Physics-Informed Neural Networks (PINN) project. It serves as a comprehensive reference for AI assistants and developers working on this codebase.

> **ìš”ì•½: ë³¸ ë³´ê³ ì„œëŠ” 'ì„ í–‰PF' ë‹¨ê³„ì˜ ìž¥ê¸° ì‚¬ì´í´ ë°ì´í„°ì™€ 'ìƒí’ˆí™”' ë‹¨ê³„ì˜ ì´ˆê¸° ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬, ìƒí’ˆí™” ì…€ì˜ ìµœì¢… ìˆ˜ëª…(1600 ì‚¬ì´í´)ì„ ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•˜ëŠ” ë²”ìš© AI ëª¨ë¸ ê°œë°œì„ ìœ„í•œ í¬ê´„ì ì¸ ê¸°ìˆ  ë¡œë“œë§µì„ ì œì‹œí•©ë‹ˆë‹¤. ì´ ëª¨ë¸ì˜ í•µì‹¬ ëª©í‘œëŠ” ì‚¬ì´í´ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ë³€í•˜ëŠ” ì „ì•• ë²”ìœ„, ìˆ˜ëª… í‰ê°€ íŒ¨í„´, ë°©ì „ í”„ë¡œí† ì½œ ë“± ê·¹ì‹¬í•œ ìš´ì˜ ì¡°ê±´ì˜ ë³€í™”ì—ë„ ê°•ê±´í•œ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ìœ ì§€í•˜ëŠ” ê²ƒìž…ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´, ë³¸ ë¡œë“œë§µì€ (1) í”„ë¡œí† ì½œ-ì¸ì§€ íŠ¹ì§• ê³µí•™, (2) ì¡°ê±´ë¶€ AI ì•„í‚¤í…ì²˜, (3) ë¬¼ë¦¬-ì§€ë„ ì „ì´ í•™ìŠµì´ë¼ëŠ” ì„¸ ê°€ì§€ í•µì‹¬ ì „ëžµì„ ìœ ê¸°ì ìœ¼ë¡œ ê²°í•©í•œ í”„ë ˆìž„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ìµœì¢…ì ìœ¼ë¡œëŠ” ë‹¤ì–‘í•œ ì…€ ì„¤ê³„ì™€ í‰ê°€ ì¡°ê±´ì˜ ë³€í™”ë¥¼ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•˜ê³  ì ì‘í•˜ì—¬, ê°œë°œ ê¸°ê°„ì„ ë‹¨ì¶•í•˜ê³  ì˜ˆì¸¡ ì •í™•ë„ë¥¼ ê·¹ëŒ€í™”í•˜ëŠ” ì§€ëŠ¥í˜• ìˆ˜ëª… ì˜ˆì¸¡ ì‹œìŠ¤í…œ êµ¬ì¶•ì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.**

---

## 1. ì„œë¡ : ë„ì „ ê³¼ì œì™€ ì „ëžµì  ì ‘ê·¼ ðŸŽ¯

ë¦¬íŠ¬ì´ì˜¨ ë°°í„°ë¦¬ì˜ ê°œë°œ ê³¼ì •ì—ì„œ 'ì„ í–‰PF' ë‹¨ê³„ëŠ” ì„¤ê³„ ë§ˆì§„ì„ ê·¹í•œìœ¼ë¡œ ì‹œí—˜í•˜ëŠ” ê°€í˜¹í•œ ì¡°ê±´ì—ì„œ ì§„í–‰ë˜ëŠ” ë°˜ë©´, 'ìƒí’ˆí™”' ë‹¨ê³„ëŠ” ì•ˆì •ì„±ê³¼ ì‹ ë¢°ì„±ì„ í™•ë³´í•˜ê¸° ìœ„í•´ ë³´ë‹¤ ì™„í™”ëœ ì¡°ê±´ìœ¼ë¡œ ìš´ì˜ë©ë‹ˆë‹¤. ì´ ë‘ ë‹¨ê³„ ì‚¬ì´ì—ëŠ” ì…€ì˜ ë¬¼ë¦¬ì  ì‚¬ì–‘(ì „ê·¹ ë¡œë”©, ë°€ë„ ë“±)ë¿ë§Œ ì•„ë‹ˆë¼, ìˆ˜ëª… í‰ê°€ë¥¼ ìœ„í•œ ì „ê¸°ì  í”„ë¡œí† ì½œ(ì „ì•• ìƒ/í•˜í•œ, C-rate, ì¶©/ë°©ì „ ìŠ¤í…)ì— ì¤‘ëŒ€í•œ ì°¨ì´ê°€ ë°œìƒí•©ë‹ˆë‹¤. íŠ¹ížˆ, ì‚¬ì´í´ì´ ì§„í–‰ë¨ì— ë”°ë¼ ì¶©ì „ ì „ì••ì„ ë™ì ìœ¼ë¡œ í•˜í–¥ ì¡°ì •í•˜ëŠ” ì „ëžµì€ ë…¸í™” ë©”ì»¤ë‹ˆì¦˜ ìžì²´ì— ì˜í–¥ì„ ë¯¸ì¹˜ë¯€ë¡œ, ê¸°ì¡´ì˜ ë°ì´í„° ê¸°ë°˜ ì˜ˆì¸¡ ëª¨ë¸ë¡œëŠ” ì´ëŸ¬í•œ ë³€í™”ë¥¼ ì˜ˆì¸¡í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.

ë³¸ ë¡œë“œë§µì€ ì´ëŸ¬í•œ **'ë„ë©”ì¸ ì´ë™(Domain Shift)'** ë¬¸ì œë¥¼ ì •ë©´ìœ¼ë¡œ í•´ê²°í•˜ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ ë‹¤ê°ì ì¸ ì ‘ê·¼ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.

* **í”„ë¡œí† ì½œ-ì¸ì§€ íŠ¹ì§• ê³µí•™ (Protocol-Aware Feature Engineering):** ë³€í™”í•˜ëŠ” ìš´ì˜ ì¡°ê±´ì„ ë¬´ì‹œí•˜ëŠ” ëŒ€ì‹ , ì´ë¥¼ ëª…ì‹œì ì¸ 'í”„ë¡œí† ì½œ ë²¡í„°'ë¡œ ì •ì˜í•˜ê³  ëª¨ë¸ì˜ í•µì‹¬ ìž…ë ¥ìœ¼ë¡œ í™œìš©í•˜ì—¬, ì¡°ê±´ì˜ ë³€í™”ê°€ ë…¸í™”ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ëª¨ë¸ì´ ì§ì ‘ í•™ìŠµí•˜ë„ë¡ í•©ë‹ˆë‹¤.
* **ì¡°ê±´ë¶€ AI ì•„í‚¤í…ì²˜ (Conditional AI Architecture):** í‘œì¤€ì ì¸ AI ëª¨ë¸ì„ ë„˜ì–´, íŠ¹ì • í”„ë¡œí† ì½œ ì¡°ê±´ í•˜ì—ì„œ ë°°í„°ë¦¬ì˜ ê±´ê°• ìƒíƒœê°€ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ì˜ 'ê´€ê³„' ìžì²´ë¥¼ í•™ìŠµí•˜ëŠ” ì¡°ê±´ë¶€(Conditional) ëª¨ë¸ì„ ë„ìž…í•©ë‹ˆë‹¤.
* **ë¬¼ë¦¬-ì§€ë„ ì „ì´ í•™ìŠµ (Physics-Guided Transfer Learning):** ë°ì´í„°ê°€ í’ë¶€í•œ ì„ í–‰PF ë°ì´í„°ë¡œ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ìƒí’ˆí™” ë°ì´í„°ì— ë¯¸ì„¸ ì¡°ì •í•˜ë˜, ë‹¨ìˆœí•œ ë°ì´í„° íŒ¨í„´ ì „ì‚¬ë¥¼ ë„˜ì–´ ë¬¼ë¦¬ ë²•ì¹™ì— ê¸°ë°˜í•œ ìžê¸° ì§€ë„ í•™ìŠµ(Self-Supervised Learning)ì„ í†µí•´ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ê³¼ ë¬¼ë¦¬ì  íƒ€ë‹¹ì„±ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.

---

## 2. 1ë‹¨ê³„: í”„ë¡œí† ì½œ-ì¸ì§€ íŠ¹ì§• ê³µí•™ ë° ë°ì´í„° êµ¬ì¡°í™” ðŸ”¬

ëª¨ë¸ì˜ ì„±ëŠ¥ì€ ìž…ë ¥ ë°ì´í„°ì˜ ì§ˆì— ì˜í•´ ê²°ì •ë©ë‹ˆë‹¤. ë³¸ ë‹¨ê³„ì˜ ëª©í‘œëŠ” ë™ì ì¸ í”„ë¡œí† ì½œ ë³€í™” ì†ì—ì„œë„ ì¼ê´€ë˜ê³  ê°•ê±´í•œ ê±´ê°• ì§€í‘œ(Health Indicator, HI)ë¥¼ ì¶”ì¶œí•˜ê³ , ëª¨ë“  ì •ë³´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ êµ¬ì¡°í™”í•˜ëŠ” ê²ƒìž…ë‹ˆë‹¤.

### 2.1. í†µí•© ë°ì´í„°ë² ì´ìŠ¤ ë° í”„ë¡œí† ì½œ ë²¡í„° ì •ì˜

ëª¨ë“  ì‹¤í—˜ ë°ì´í„°ëŠ” ì‚¬ì´í´ ë²ˆí˜¸ë¥¼ í‚¤(key)ë¡œ í•˜ëŠ” í†µí•© ë°ì´í„°ë² ì´ìŠ¤ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤. ê° ì‚¬ì´í´ ë°ì´í„°ëŠ” **(1) ì›ì‹œ ì‹œê³„ì—´ ë°ì´í„°**(ì „ì••, ì „ë¥˜, ë‘ê»˜, ì‹œê°„), **(2) ì¶”ì¶œëœ ê±´ê°• ì§€í‘œ(HI) ë²¡í„°**, ê·¸ë¦¬ê³  **(3) í”„ë¡œí† ì½œ ë²¡í„°**ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

**í”„ë¡œí† ì½œ ë²¡í„° (Protocol Vector):** í•´ë‹¹ ì‚¬ì´í´ì˜ ìš´ì˜ ì¡°ê±´ì„ ì •ëŸ‰í™”í•œ ë²¡í„°ë¡œ, ëª¨ë¸ì´ ì¡°ê±´ ë³€í™”ë¥¼ ì¸ì§€í•˜ëŠ” í•µì‹¬ ì—­í• ì„ í•©ë‹ˆë‹¤.
* `[V_max, V_min, Charge_C_rate, Cutoff_Current, Discharge_Protocol_Type, Num_Charge_Steps, ...]`

### 2.2. ê°•ê±´í•œ ê±´ê°• ì§€í‘œ(HI) ì¶”ì¶œ ì „ëžµ

**1. ê¸°ì¤€ HI ì¶”ì¶œ (ì €ë¹ˆë„, 100 ì‚¬ì´í´ë§ˆë‹¤):**
0.2C ì €ìœ¨ ì¶©/ë°©ì „ ì‚¬ì´í´ì€ í”„ë¡œí† ì½œ ë³€í™”ì™€ ë¬´ê´€í•˜ê²Œ ìˆ˜í–‰ë˜ëŠ” 'ì ˆëŒ€ ê¸°ì¤€'ìž…ë‹ˆë‹¤. ì´ ë°ì´í„°ë¡œë¶€í„° ê°€ìž¥ ì‹ ë¢°ë„ ë†’ì€ HIë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
* **ë¯¸ë¶„ìš©ëŸ‰ë¶„ì„ (dQ/dV):** ì „ì••-ìš©ëŸ‰ ê³¡ì„ ì„ ë¯¸ë¶„í•˜ì—¬ ë…¸í™” ë©”ì»¤ë‹ˆì¦˜(ë¦¬íŠ¬ ì†ì‹¤, í™œë¬¼ì§ˆ ì†ì‹¤ ë“±)ê³¼ ì§ì ‘ì ìœ¼ë¡œ ì—°ê´€ëœ í”¼í¬ì˜ ìœ„ì¹˜, ë†’ì´, ë©´ì  ë³€í™”ë¥¼ ì¶”ì í•©ë‹ˆë‹¤.
* **ì§ë¥˜ ë‚´ë¶€ ì €í•­ (DCIR):** ì •ì „ë¥˜ ì¶©ì „ ì‹œìž‘ ì‹œì ì˜ ì „ì•• ê°•í•˜ë¥¼ í†µí•´ ê³„ì‚°í•˜ë©°, ì „í•´ì•¡ ë° ê³„ë©´ ì €í•­ ì¦ê°€ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ë°˜ì˜í•©ë‹ˆë‹¤.
* **ì…€ ë‘ê»˜ ì¦ê°€ìœ¨:** ì „ê·¹ íŒ½ì°½ ë° ê°€ìŠ¤ ë°œìƒìœ¼ë¡œ ì¸í•œ ë¹„ê°€ì—­ì  ë³€í™”ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.

**2. ê°€ë³€ ì „ì•• ë²”ìœ„ dQ/dV ë¶„ì„ ë¬¸ì œ í•´ê²°: 'ê´€ì‹¬ ì˜ì—­ ë¶„ì„'**
ë™ì ìœ¼ë¡œ ë³€í•˜ëŠ” ì „ì•• ë²”ìœ„ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ëª¨ë“  í”„ë¡œí† ì½œì— **ê³µí†µì ìœ¼ë¡œ í¬í•¨ë˜ëŠ” ì „ì•• êµ¬ê°„(ì˜ˆ: 3.30V ~ 4.39V)ì„ 'ê´€ì‹¬ ì˜ì—­(Region of Interest, ROI)'ìœ¼ë¡œ ì„¤ì •**í•©ë‹ˆë‹¤. ëª¨ë“  dQ/dV íŠ¹ì§•(í”¼í¬ ìœ„ì¹˜, ë†’ì´, ë©´ì  ë“±)ì€ ì´ ê³ ì •ëœ ROI ë‚´ì—ì„œë§Œ ê³„ì‚°í•˜ì—¬, ì „ì•• ë²”ìœ„ ë³€í™”ì— ê´€ê³„ì—†ì´ ì¼ê´€ëœ ê¸°ì¤€ìœ¼ë¡œ íŠ¹ì§•ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

**3. ë³´ì¡° HI ì¶”ì¶œ ë° ë°ì´í„° ìœµí•© (ê³ ë¹ˆë„, ë§¤ ì‚¬ì´í´):**
ë§¤ ì‚¬ì´í´ ìˆ˜í–‰ë˜ëŠ” ê³ ìœ¨ ì¶©/ë°©ì „ ë°ì´í„°ë¡œë¶€í„° ë³´ì¡° HIë¥¼ ì¶”ì¶œí•˜ê³ , ì €ë¹ˆë„ ê¸°ì¤€ HIì™€ ìœµí•©í•˜ì—¬ ì¡°ë°€í•œ(dense) ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
* **ì¶”ì¶œ:** ì •ì „ë¥˜(CC)/ì •ì „ì••(CV) êµ¬ê°„ì˜ ì§€ì† ì‹œê°„, íŠ¹ì • ì „ì•• êµ¬ê°„ ë„ë‹¬ ì‹œê°„ ë“±.
* **ìœµí•©:** **ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ íšŒê·€(Gaussian Process Regression, GPR)**ë¥¼ ì‚¬ìš©í•˜ì—¬ ì €ë¹ˆë„ ê¸°ì¤€ HIë¥¼ ë³´ê°„(interpolate)í•©ë‹ˆë‹¤. GPRì€ ë¶ˆí™•ì‹¤ì„± ì˜ˆì¸¡ì´ ê°€ëŠ¥í•˜ì—¬, ë³´ê°„ëœ ê°’ì˜ ì‹ ë¢°ë„ë¥¼ ëª¨ë¸ í•™ìŠµì— í•¨ê»˜ í™œìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

---

## 3. 2ë‹¨ê³„: ì¡°ê±´ë¶€ AI ì•„í‚¤í…ì²˜ ì„¤ê³„ ðŸ§ 

ë³¸ í”„ë ˆìž„ì›Œí¬ì˜ í•µì‹¬ì€ 'í”„ë¡œí† ì½œ ë²¡í„°'ë¥¼ ì¡°ê±´ìœ¼ë¡œ ìž…ë ¥ë°›ì•„, ë…¸í™” ì˜ˆì¸¡ì„ ë™ì ìœ¼ë¡œ ì¡°ì ˆí•˜ëŠ” **ì¡°ê±´ë¶€(Conditional) AI ëª¨ë¸**ì„ ì„¤ê³„í•˜ëŠ” ê²ƒìž…ë‹ˆë‹¤. ìµœì‹  ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ì¸ Transformer ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ì™¸ë¶€ ë³€ìˆ˜ ì²˜ë¦¬ ëŠ¥ë ¥ì´ ê²€ì¦ëœ ëª¨ë¸ë“¤ì˜ ì•„ì´ë””ì–´ë¥¼ ìœµí•©í•©ë‹ˆë‹¤.

### 3.1. ì¡°ê±´ë¶€ íŠ¸ëžœìŠ¤í¬ë¨¸ (Conditional Transformer) ì•„í‚¤í…ì²˜

* **ê¸°ë³¸ êµ¬ì¡°:** Transformer ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°ë¥¼ ì±„íƒí•˜ì—¬, ê³¼ê±°ì˜ HI ì‹œê³„ì—´ê³¼ í”„ë¡œí† ì½œ ì‹œê³„ì—´ì„ ë°”íƒ•ìœ¼ë¡œ ë¯¸ëž˜ì˜ HI ì‹œê³„ì—´ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
* **ì¡°ê±´ë¶€ ì •ë³´ í†µí•©:**
    1.  **ìž…ë ¥ ìž„ë² ë”©:** 'HI ë²¡í„°'ì™€ 'í”„ë¡œí† ì½œ ë²¡í„°'ë¥¼ ê°ê° ë³„ë„ì˜ ìž„ë² ë”© ë ˆì´ì–´ë¥¼ í†µí•´ ê³ ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    2.  **Cross-Attention ë©”ì»¤ë‹ˆì¦˜:** Transformerì˜ í•µì‹¬ì¸ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ í™œìš©í•˜ì—¬, HI ì‹œí€€ìŠ¤ì™€ í”„ë¡œí† ì½œ ì‹œí€€ìŠ¤ ê°„ì˜ ìƒí˜¸ìž‘ìš©ì„ ëª¨ë¸ë§í•©ë‹ˆë‹¤. ì¦‰, íŠ¹ì • ì‹œì ì˜ HIë¥¼ ì²˜ë¦¬í•  ë•Œ, ê´€ë ¨ëœ í”„ë¡œí† ì½œ ì •ë³´(ì˜ˆ: í˜„ìž¬ì˜ Vmax ê°’)ì— ë” ë§Žì€ 'ì£¼ì˜(attention)'ë¥¼ ê¸°ìš¸ì´ë„ë¡ í•™ìŠµí•©ë‹ˆë‹¤.
    3.  **FiLM (Feature-wise Linear Modulation) ì ìš©:** Cross-Attentionê³¼ ë”ë¶ˆì–´, í”„ë¡œí† ì½œ ë²¡í„°ë¡œë¶€í„° ìƒì„±ëœ íŒŒë¼ë¯¸í„°(Î³, Î²)ë¥¼ ì‚¬ìš©í•˜ì—¬ Transformer ë¸”ë¡ì˜ ì¤‘ê°„ í”¼ì²˜ë§µì„ ì§ì ‘ ì„ í˜• ë³€ì¡°(affine transformation)í•©ë‹ˆë‹¤. ì´ëŠ” í”„ë¡œí† ì½œ ì¡°ê±´ì— ë”°ë¼ ëª¨ë¸ì˜ ì—°ì‚° ë°©ì‹ì„ ë”ìš± ë¯¸ì„¸í•˜ê³  ì§ì ‘ì ìœ¼ë¡œ ì œì–´í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.

ì´ëŸ¬í•œ êµ¬ì¡°ë¥¼ í†µí•´ ëª¨ë¸ì€ "Vmaxê°€ 4.50Vì¼ ë•Œì˜ ìš©ëŸ‰ ê°ì†Œìœ¨"ê³¼ "Vmaxê°€ 4.44Vë¡œ í•˜í–¥ ì¡°ì •ë˜ì—ˆì„ ë•Œì˜ ìš©ëŸ‰ ê°ì†Œìœ¨"ì´ ì–´ë–»ê²Œ ë‹¤ë¥¸ì§€ë¥¼ ë°ì´í„°ë¡œë¶€í„° í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤.

---

## 4. 3ë‹¨ê³„: ë¬¼ë¦¬-ì§€ë„ ì „ì´ í•™ìŠµ ë° ê²€ì¦ ðŸš€

ê°€ìž¥ ì¤‘ìš”í•œ ë‹¨ê³„ë¡œ, ì„ í–‰PF ë°ì´í„°ë¡œ í•™ìŠµí•œ ì§€ì‹ì„ ìƒí’ˆí™” ì…€ì— íš¨ê³¼ì ìœ¼ë¡œ ì´ì „í•˜ê³ , ë¬¼ë¦¬ì  íƒ€ë‹¹ì„±ì„ í™•ë³´í•˜ì—¬ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.

### 4.1. 2ë‹¨ê³„ ì „ì´ í•™ìŠµ ì „ëžµ

1.  **ì‚¬ì „ í•™ìŠµ (Pre-training):** ë°ì´í„°ê°€ í’ë¶€í•œ 'ì„ í–‰PF' ë°ì´í„°ì…‹(1000+ ì‚¬ì´í´)ì„ ì‚¬ìš©í•˜ì—¬ 2ë‹¨ê³„ì—ì„œ ì„¤ê³„í•œ ì¡°ê±´ë¶€ íŠ¸ëžœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ ì‚¬ì „ í•™ìŠµì‹œí‚µë‹ˆë‹¤. ì´ ë‹¨ê³„ì—ì„œ ëª¨ë¸ì€ LCO/Gr-SiC ì‹œìŠ¤í…œì˜ ê¸°ë³¸ì ì¸ ë…¸í™” ë¬¼ë¦¬í•™ê³¼ ë‹¤ì–‘í•œ í”„ë¡œí† ì½œ ì¡°ê±´ì— ëŒ€í•œ ë°˜ì‘ì„ í•™ìŠµí•©ë‹ˆë‹¤.
2.  **ë¯¸ì„¸ ì¡°ì • (Fine-tuning):** ì œí•œëœ 'ìƒí’ˆí™”' ë°ì´í„°(ì´ˆê¸° 200~300 ì‚¬ì´í´)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•©ë‹ˆë‹¤. ì´ë•Œ, ëª¨ë¸ì˜ í•˜ìœ„ ë ˆì´ì–´(ê¸°ë³¸ ë¬¼ë¦¬ íŠ¹ì„± í•™ìŠµ)ëŠ” ìž‘ì€ í•™ìŠµë¥ ë¡œ, ìƒìœ„ ë ˆì´ì–´(ì…€ íŠ¹ì´ì  íŠ¹ì„± í•™ìŠµ)ëŠ” í° í•™ìŠµë¥ ë¡œ ì—…ë°ì´íŠ¸í•˜ëŠ” **ì°¨ë“± í•™ìŠµë¥ (Discriminative Learning Rates)** ê¸°ë²•ì„ ì ìš©í•˜ì—¬, ì‚¬ì „ í•™ìŠµëœ ì§€ì‹ì˜ ì†ì‹¤(catastrophic forgetting)ì„ ìµœì†Œí™”í•˜ê³  ìƒˆë¡œìš´ ë„ë©”ì¸ì— ë¹ ë¥´ê²Œ ì ì‘ì‹œí‚µë‹ˆë‹¤.

### 4.2. ë¬¼ë¦¬-ì§€ë„ ìžê¸° ì§€ë„ í•™ìŠµ (Physics-Guided Self-Supervised Learning)

ë¯¸ì„¸ ì¡°ì • ë‹¨ê³„ì—ì„œ, ë‹¨ìˆœížˆ ìš©ëŸ‰ ì˜ˆì¸¡ ì˜¤ì°¨(Supervised Loss)ë§Œì„ ìµœì†Œí™”í•˜ëŠ” ê²ƒì„ ë„˜ì–´, ë¬¼ë¦¬ ë²•ì¹™ì— ê¸°ë°˜í•œ **ìžê¸° ì§€ë„ í•™ìŠµ ì†ì‹¤(Self-Supervised Loss)**ì„ ì¶”ê°€í•©ë‹ˆë‹¤. ì´ëŠ” ë ˆì´ë¸”ì´ ì—†ëŠ” ë°ì´í„°ë¡œë¶€í„° ëª¨ë¸ì´ ìŠ¤ìŠ¤ë¡œ ë¬¼ë¦¬ì  ì¼ê´€ì„±ì„ í•™ìŠµí•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.

* **í”„ë¦¬í…ìŠ¤íŠ¸ íƒœìŠ¤í¬ (Pretext Task) ì˜ˆì‹œ:**
    * **ë¬¼ë¦¬ì  ì¼ê´€ì„± ì˜ˆì¸¡:** ëª¨ë¸ì—ê²Œ í˜„ìž¬ ì‚¬ì´í´ì˜ dQ/dV íŠ¹ì§•, DCIR, ë‘ê»˜ ë³€í™”ëŸ‰ì„ ìž…ë ¥ìœ¼ë¡œ ì£¼ê³ , ë‹¤ìŒ 100 ì‚¬ì´í´ í›„ì˜ 0.2C ê¸°ì¤€ ìš©ëŸ‰ ê°ì†ŒëŸ‰ì„ ì˜ˆì¸¡í•˜ë„ë¡ í•©ë‹ˆë‹¤. ì´ ì˜¤ì°¨ë¥¼ ì†ì‹¤ í•¨ìˆ˜ì— ì¶”ê°€í•˜ì—¬, ëª¨ë¸ì´ ì „ê¸°í™”í•™ì  íŠ¹ì§•ê³¼ ë¬¼ë¦¬ì  íŒ½ì°½, ì‹¤ì œ ìš©ëŸ‰ ê°ì†Œ ê°„ì˜ ì •ëŸ‰ì  ê´€ê³„ë¥¼ í•™ìŠµí•˜ë„ë¡ ê°•ì œí•©ë‹ˆë‹¤.
    * **í”„ë¡œí† ì½œ ë¯¼ê°ë„ ì˜ˆì¸¡:** ë™ì¼í•œ HI ìƒíƒœë¥¼ ê°€ì§„ ë‘ ê°œì˜ ê°€ìƒ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë§Œë“¤ê³ , í•˜ë‚˜ì˜ í”„ë¡œí† ì½œ ë²¡í„°ì—ëŠ” ë†’ì€ Vmaxë¥¼, ë‹¤ë¥¸ í•˜ë‚˜ì—ëŠ” ë‚®ì€ Vmaxë¥¼ ìž…ë ¥í•©ë‹ˆë‹¤. ëª¨ë¸ì´ ë‘ ì‹œë‚˜ë¦¬ì˜¤ ê°„ì˜ ì˜ˆìƒ ìˆ˜ëª… ì°¨ì´ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•˜ê³ , ì´ ì°¨ì´ê°€ ë¬¼ë¦¬ì ìœ¼ë¡œ íƒ€ë‹¹í•œ ë°©í–¥ê³¼ í¬ê¸°ë¥¼ ê°–ë„ë¡ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì„¤ê³„í•©ë‹ˆë‹¤.

### 4.3. ê²€ì¦ ë° í‰ê°€

* ë¯¸ì„¸ ì¡°ì •ëœ ìµœì¢… ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìƒí’ˆí™” ì…€ì˜ 1600 ì‚¬ì´í´ ì‹œì  SOH ë° ì „ì²´ ìš©ëŸ‰ ê°ì†Œ ê³¡ì„ ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
* ì¶”í›„ í™•ë³´ë  ì‹¤ì œ 1600 ì‚¬ì´í´ ì‹¤í—˜ ë°ì´í„°ì™€ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë¹„êµí•˜ì—¬, í‰ê·  ì ˆëŒ€ ë°±ë¶„ìœ¨ ì˜¤ì°¨(MAPE), í‰ê·  ì œê³±ê·¼ ì˜¤ì°¨(RMSE) ë“±ì˜ ì§€í‘œë¡œ ëª¨ë¸ì˜ ìµœì¢… ì„±ëŠ¥ì„ ì •ëŸ‰ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.

---

## 5. ê²°ë¡  ë° ê¸°ëŒ€íš¨ê³¼ ðŸŒŸ

ë³¸ ë³´ê³ ì„œì—ì„œ ì œì•ˆí•˜ëŠ” ë¡œë“œë§µì€ ë™ì ìœ¼ë¡œ ë³€í™”í•˜ëŠ” ë³µìž¡í•œ ìš´ì˜ ì¡°ê±´ í•˜ì—ì„œ ë°°í„°ë¦¬ ìˆ˜ëª…ì„ ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ ì²´ê³„ì ì´ê³  ë‹¤ê°ì ì¸ ì ‘ê·¼ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. í”„ë¡œí† ì½œ ì •ë³´ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ê³ , ë¬¼ë¦¬ ë²•ì¹™ì„ í•™ìŠµ ê³¼ì •ì— í†µí•©í•¨ìœ¼ë¡œì¨, ë³¸ í”„ë ˆìž„ì›Œí¬ëŠ” ê¸°ì¡´ ë°ì´í„° ê¸°ë°˜ ëª¨ë¸ì˜ í•œê³„ë¥¼ ë›°ì–´ë„˜ëŠ” ê°•ê±´í•¨ê³¼ ì¼ë°˜í™” ì„±ëŠ¥ì„ í™•ë³´í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

**ê¸°ëŒ€ íš¨ê³¼:**
* **ê°œë°œ ê¸°ê°„ ë‹¨ì¶•:** ìƒí’ˆí™” ë‹¨ê³„ì˜ ìž¥ê¸° ìˆ˜ëª… í‰ê°€ê°€ ì™„ë£Œë˜ê¸° ì „, ì´ˆê¸° ë°ì´í„°ë§Œìœ¼ë¡œ ìµœì¢… ìˆ˜ëª…ì„ ë†’ì€ ì •í™•ë„ë¡œ ì˜ˆì¸¡í•˜ì—¬ ê°œë°œ ì˜ì‚¬ê²°ì •ì„ ê°€ì†í™”í•©ë‹ˆë‹¤.
* **ì •í™•ë„ í–¥ìƒ:** ì„ í–‰PF ë‹¨ê³„ì˜ í’ë¶€í•œ ë°ì´í„°ì™€ ë¬¼ë¦¬ì  ì´í•´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒí’ˆí™” ì…€ì˜ ë…¸í™” ê±°ë™ì„ ì˜ˆì¸¡í•¨ìœ¼ë¡œì¨, ë‹¨ìˆœ ì™¸ì‚½(extrapolation) ë°©ì‹ ëŒ€ë¹„ ì˜ˆì¸¡ ì •í™•ë„ë¥¼ íšê¸°ì ìœ¼ë¡œ ê°œì„ í•©ë‹ˆë‹¤.
* **ë²”ìš©ì„± í™•ë³´:** í–¥í›„ ìƒˆë¡œìš´ ì†Œìž¬ë‚˜ í‰ê°€ í”„ë¡œí† ì½œì´ ë„ìž…ë˜ë”ë¼ë„, í”„ë ˆìž„ì›Œí¬ì˜ ì „ì´ í•™ìŠµ ë° ì§€ì†ì  í•™ìŠµ ê¸°ëŠ¥ì„ í†µí•´ ìƒˆë¡œìš´ ì¡°ê±´ì— ë¹ ë¥´ê²Œ ì ì‘í•˜ëŠ” ë²”ìš© ì˜ˆì¸¡ í”Œëž«í¼ìœ¼ë¡œ ë°œì „í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

## Project Overview

This project implements Physics-Informed Neural Networks for solving partial differential equations (PDEs) using deep learning. The implementation consists of two main modules:

1. **pinn**: Core PINN implementation with neural network architectures, training algorithms, and PDE solvers
2. **preprocess**: Data preprocessing toolkit for domain generation, sampling, and data preparation

## Architecture Guidelines

### Module Organization

```
project_ryu/
â”œâ”€â”€ pinn/                      # Core PINN implementation
â”‚   â”œâ”€â”€ models/               # Neural network architectures
â”‚   â”œâ”€â”€ losses/               # Loss functions (PDE, boundary, data)
â”‚   â”œâ”€â”€ training/             # Training loops and optimizers
â”‚   â”œâ”€â”€ utils/                # Utilities (sampling, visualization)
â”‚   â””â”€â”€ examples/             # Example implementations
â”œâ”€â”€ preprocess/               # Data preprocessing module
â”‚   â”œâ”€â”€ generators/           # Domain and mesh generation
â”‚   â”œâ”€â”€ samplers/             # Sampling strategies
â”‚   â”œâ”€â”€ transformers/         # Data transformation utilities
â”‚   â”œâ”€â”€ io/                   # Input/output handlers
â”‚   â””â”€â”€ validation/           # Data validation tools
â””â”€â”€ tests/                    # Test suites for both modules
```

### Design Principles

1. **Modularity**: Each component should be self-contained and reusable
2. **Extensibility**: Easy to add new PDEs, architectures, and sampling methods
3. **Performance**: Optimize for GPU acceleration and large-scale problems
4. **Robustness**: Comprehensive error handling and validation
5. **Documentation**: All public APIs must be well-documented

## Implementation Standards

### Code Style

```python
# Use type hints for all function signatures
from typing import List, Tuple, Optional, Callable, Dict, Any
import torch
import numpy as np

def train_model(
    model: torch.nn.Module,
    data: Dict[str, torch.Tensor],
    epochs: int = 1000,
    learning_rate: float = 1e-3,
    device: Optional[str] = None
) -> Dict[str, List[float]]:
    """
    Train a PINN model.
    
    Args:
        model: The neural network model
        data: Dictionary containing training data
        epochs: Number of training epochs
        learning_rate: Learning rate for optimizer
        device: Device to train on ('cpu', 'cuda', etc.)
    
    Returns:
        Dictionary containing training history
    """
    # Implementation here
    pass
```

### Error Handling

```python
# Always validate inputs
def create_domain(bounds: List[Tuple[float, float]]) -> Domain:
    if not bounds:
        raise ValueError("Bounds cannot be empty")
    
    for i, (low, high) in enumerate(bounds):
        if low >= high:
            raise ValueError(f"Invalid bounds for dimension {i}: [{low}, {high}]")
    
    # Create domain...
```

### Testing Requirements

- Unit tests for all core functionality
- Integration tests for end-to-end workflows
- Performance benchmarks for critical operations
- Coverage target: >90% for core modules

## Core Component Implementation

### PINN Neural Network

```python
class PINN(nn.Module):
    """Base class for Physics-Informed Neural Networks."""
    
    def __init__(self, ...):
        super().__init__()
        # Initialize layers with proper weight initialization
        self._build_network()
        self._initialize_weights()
    
    def _build_network(self):
        """Construct the neural network architecture."""
        # Use ModuleList for dynamic architectures
        self.layers = nn.ModuleList()
        # Build layers based on configuration
    
    def _initialize_weights(self):
        """Initialize network weights using Xavier or He initialization."""
        for layer in self.layers:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)
                nn.init.zeros_(layer.bias)
```

### PDE Loss Implementation

```python
class PDELoss:
    """Compute PDE residual loss using automatic differentiation."""
    
    def __init__(self, pde_function: Callable):
        self.pde_function = pde_function
    
    def compute_loss(
        self,
        model: PINN,
        collocation_points: torch.Tensor
    ) -> torch.Tensor:
        """
        Compute PDE residual at collocation points.
        
        Key implementation details:
        1. Enable gradient computation with create_graph=True
        2. Handle multiple output dimensions properly
        3. Ensure numerical stability in derivative computation
        """
        # Set requires_grad for inputs
        collocation_points.requires_grad_(True)
        
        # Forward pass
        predictions = model(collocation_points)
        
        # Compute PDE residual
        residual = self.pde_function(predictions, collocation_points, model)
        
        # Return mean squared residual
        return torch.mean(residual**2)
```

### Adaptive Sampling Strategy

```python
class AdaptiveSampler:
    """Implement adaptive sampling based on PDE residuals."""
    
    def sample(
        self,
        n_points: int,
        current_residuals: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Sample points with higher density in high-residual regions.
        
        Algorithm:
        1. If no residuals provided, use uniform sampling
        2. Otherwise, compute probability density from residuals
        3. Use importance sampling to generate new points
        4. Ensure minimum coverage of entire domain
        """
        # Implementation details
        pass
```

## Performance Optimization

### GPU Utilization

```python
# Always check for GPU availability
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Use mixed precision training for better performance
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()
with autocast():
    loss = compute_loss(model, data)

# Optimize memory usage
torch.cuda.empty_cache()  # Clear unused memory
```

### Batch Processing

```python
def train_batch(model, batch_data, optimizer):
    """Process data in batches for memory efficiency."""
    # Use DataLoader for automatic batching
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        pin_memory=True,  # For GPU transfer
        num_workers=4     # Parallel data loading
    )
```

### Vectorization

```python
# Prefer vectorized operations over loops
# Bad:
residuals = []
for point in points:
    residual = compute_residual(point)
    residuals.append(residual)

# Good:
residuals = compute_residual_vectorized(points)  # Process all points at once
```

## Common Patterns

### Factory Pattern for PDEs

```python
class PDEFactory:
    """Factory for creating PDE instances."""
    
    _registry = {}
    
    @classmethod
    def register(cls, name: str, pde_class: type):
        """Register a new PDE type."""
        cls._registry[name] = pde_class
    
    @classmethod
    def create(cls, name: str, **kwargs):
        """Create a PDE instance by name."""
        if name not in cls._registry:
            raise ValueError(f"Unknown PDE: {name}")
        return cls._registry[name](**kwargs)

# Usage
PDEFactory.register('heat', HeatEquation)
pde = PDEFactory.create('heat', alpha=0.01)
```

### Configuration Management

```python
from dataclasses import dataclass
from typing import List, Optional

@dataclass
class PINNConfig:
    """Configuration for PINN model."""
    input_dim: int
    hidden_dims: List[int]
    output_dim: int
    activation: str = 'tanh'
    dropout_rate: float = 0.0
    use_batch_norm: bool = False
    
    def validate(self):
        """Validate configuration parameters."""
        if self.input_dim <= 0:
            raise ValueError("input_dim must be positive")
        # Additional validation...
```

## Integration Points

### PINN-Preprocess Integration

```python
# Standard workflow for data preparation
from preprocess import PreprocessingPipeline
from pinn import PINN

# 1. Setup preprocessing pipeline
pipeline = PreprocessingPipeline()
pipeline.add_step('generate', DomainGenerator(...))
pipeline.add_step('sample', AdaptiveSampler(...))
pipeline.add_step('normalize', Normalizer(...))

# 2. Process data
data = pipeline.run()

# 3. Create and train PINN
model = PINN(config)
trainer = Trainer(model, data)
history = trainer.train()
```

### External Library Integration

When integrating external libraries:

1. **Check compatibility**: Ensure version compatibility
2. **Wrap interfaces**: Create wrapper classes for external APIs
3. **Handle dependencies**: Use optional imports for non-critical features
4. **Document requirements**: Clearly list all dependencies

## Debugging and Troubleshooting

### Common Issues

1. **Gradient Explosion/Vanishing**
   - Use gradient clipping
   - Try different activation functions
   - Check weight initialization

2. **Poor Convergence**
   - Verify PDE implementation
   - Adjust learning rate schedule
   - Balance loss weights

3. **Memory Issues**
   - Use batch processing
   - Enable gradient checkpointing
   - Reduce model size

### Debugging Tools

```python
# Enable debugging mode
torch.autograd.set_detect_anomaly(True)

# Log intermediate values
import logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

# Visualize gradients
def plot_gradients(model):
    """Visualize gradient flow through the network."""
    for name, param in model.named_parameters():
        if param.requires_grad and param.grad is not None:
            print(f"{name}: grad_norm = {param.grad.norm().item():.4f}")
```

## Future Extensions

### Planned Features

1. **Multi-GPU Support**: Distributed training across multiple GPUs
2. **Adaptive Architectures**: Dynamic network sizing based on problem complexity
3. **Uncertainty Quantification**: Bayesian neural networks for uncertainty estimates
4. **Transfer Learning**: Pre-trained models for common PDEs
5. **Real-time Inference**: Optimized inference for real-time applications

### Extension Points

- **Custom Losses**: Add new loss functions in `pinn/losses/`
- **New Architectures**: Implement in `pinn/models/architectures.py`
- **Sampling Methods**: Add to `preprocess/samplers/`
- **Data Formats**: Extend `preprocess/io/readers.py`

## Development Workflow

### Version Control

```bash
# Branch naming convention
feature/add-fourier-features
bugfix/gradient-computation
refactor/sampling-module

# Commit message format
feat: Add adaptive sampling strategy
fix: Correct gradient computation in PDE loss
docs: Update API documentation for PINN class
test: Add unit tests for boundary conditions
```

### Code Review Checklist

- [ ] Type hints for all functions
- [ ] Docstrings for public APIs
- [ ] Unit tests for new functionality
- [ ] Performance benchmarks for critical paths
- [ ] Update documentation
- [ ] Check GPU compatibility
- [ ] Verify numerical stability

## Performance Benchmarks

Target performance metrics:

- **Training Speed**: >1000 iterations/second for small problems
- **Memory Usage**: <4GB for typical 2D problems
- **Convergence**: <1e-3 relative error within 5000 epochs
- **Scaling**: Linear scaling up to 1M collocation points

## Security Considerations

1. **Input Validation**: Always validate user inputs
2. **File I/O**: Sanitize file paths and limit file access
3. **Numerical Stability**: Check for overflow/underflow
4. **Resource Limits**: Implement timeouts and memory limits

## Documentation Standards

### API Documentation

```python
def function_name(
    param1: type,
    param2: type
) -> return_type:
    """
    Brief description of function purpose.
    
    Extended description explaining implementation details,
    algorithm used, and any important considerations.
    
    Args:
        param1: Description of first parameter
        param2: Description of second parameter
    
    Returns:
        Description of return value
    
    Raises:
        ValueError: When invalid parameters are provided
        RuntimeError: When computation fails
    
    Example:
        >>> result = function_name(value1, value2)
        >>> print(result)
        expected_output
    
    Note:
        Additional notes about usage or limitations
    """
```

### Code Comments

- Use inline comments sparingly, only for non-obvious logic
- Prefer self-documenting code with clear variable names
- Add TODO comments with issue numbers for future work

## Conclusion

This document provides comprehensive guidelines for implementing and extending the PINN project. Follow these standards to ensure code quality, maintainability, and performance. Regular updates to this document should reflect new patterns and best practices discovered during development.